[
  {
    "objectID": "stan-usage.html",
    "href": "stan-usage.html",
    "title": "Usage with Stan models",
    "section": "",
    "text": "This document shows how to use nutpie with Stan models. We will use the nutpie package to define a simple model and sample from it using Stan."
  },
  {
    "objectID": "stan-usage.html#installation",
    "href": "stan-usage.html#installation",
    "title": "Usage with Stan models",
    "section": "Installation",
    "text": "Installation\nFor Stan, it is more common to use pip or uv to install the necessary packages. However, conda is also an option if you prefer.\nTo install using pip:\npip install \"nutpie[stan]\"\nTo install using uv:\nuv add \"nutpie[stan]\"\nTo install using conda:\nconda install -c conda-forge nutpie"
  },
  {
    "objectID": "stan-usage.html#compiler-toolchain",
    "href": "stan-usage.html#compiler-toolchain",
    "title": "Usage with Stan models",
    "section": "Compiler Toolchain",
    "text": "Compiler Toolchain\nStan requires a compiler toolchain to be installed on your system. This is necessary for compiling the Stan models. You can find detailed instructions for setting up the compiler toolchain in the CmdStan Guide.\nAdditionally, since Stan uses Intel’s Threading Building Blocks (TBB) for parallelism, you might need to set the TBB_CXX_TYPE environment variable to specify the compiler type. Depending on your system, you can set it to either clang or gcc. For example:\n\nimport os\nos.environ[\"TBB_CXX_TYPE\"] = \"clang\"  # or 'gcc'\n\nMake sure to set this environment variable before compiling your Stan models to ensure proper configuration."
  },
  {
    "objectID": "stan-usage.html#defining-and-sampling-a-simple-model",
    "href": "stan-usage.html#defining-and-sampling-a-simple-model",
    "title": "Usage with Stan models",
    "section": "Defining and Sampling a Simple Model",
    "text": "Defining and Sampling a Simple Model\nWe will define a simple Bayesian model using Stan and sample from it using nutpie.\n\nModel Definition\nIn your Python script or Jupyter notebook, add the following code:\n\nimport nutpie\n\nmodel_code = \"\"\"\ndata {\n    int&lt;lower=0&gt; N;\n    vector[N] y;\n}\nparameters {\n    real mu;\n}\nmodel {\n    mu ~ normal(0, 1);\n    y ~ normal(mu, 1);\n}\n\"\"\"\n\ncompiled_model = nutpie.compile_stan_model(code=model_code)\n\n\n\nSampling\nWe can now compile the model and sample from it:\n\ncompiled_model_with_data = compiled_model.with_data(N=3, y=[1, 2, 3])\ntrace = nutpie.sample(compiled_model_with_data)\n\n\n\n\n\n\n\n    Sampler Progress\n    Total Chains: 6\n    Active Chains: 0\n    \n        Finished Chains:\n        6\n    \n    Sampling for now\n    \n        Estimated Time to Completion:\n        now\n    \n\n    \n    \n    \n        \n            \n                Progress\n                Draws\n                Divergences\n                Step Size\n                Gradients/Draw\n            \n        \n        \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    1.33\n                    3\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    1.39\n                    1\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    1.37\n                    3\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    1.38\n                    1\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    1.35\n                    3\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    1.33\n                    3\n                \n            \n            \n        \n    \n\n\n\n\n\nUsing Dimensions\nWe’ll use the radon model from this case-study from the stan documentation, to show how we can use coordinates and dimension names to simplify working with trace objects.\nWe follow the same data preparation as in the case-study:\n\nimport pandas as pd\nimport numpy as np\nimport arviz as az\nimport seaborn as sns\n\nhome_data = pd.read_csv(\n    \"https://github.com/pymc-devs/pymc-examples/raw/refs/heads/main/examples/data/srrs2.dat\",\n    index_col=\"idnum\",\n)\ncounty_data = pd.read_csv(\n    \"https://github.com/pymc-devs/pymc-examples/raw/refs/heads/main/examples/data/cty.dat\",\n)\n\nradon_data = (\n    home_data\n    .rename(columns=dict(cntyfips=\"ctfips\"))\n    .merge(\n        (\n            county_data\n            .drop_duplicates(['stfips', 'ctfips', 'st', 'cty', 'Uppm'])\n            .set_index([\"ctfips\", \"stfips\"])\n        ),\n        right_index=True,\n        left_on=[\"ctfips\", \"stfips\"],\n    )\n    .assign(log_radon=lambda x: np.log(np.clip(x.activity, 0.1, np.inf)))\n    .assign(log_uranium=lambda x: np.log(np.clip(x[\"Uppm\"], 0.1, np.inf)))\n    .query(\"state == 'MN'\")\n)\n\nAnd also use the partially pooled model from the case-study:\n\nmodel_code = \"\"\"\ndata {\n  int&lt;lower=1&gt; N;  // observations\n  int&lt;lower=1&gt; J;  // counties\n  array[N] int&lt;lower=1, upper=J&gt; county;\n  vector[N] x;\n  vector[N] y;\n}\nparameters {\n  real mu_alpha;\n  real&lt;lower=0&gt; sigma_alpha;\n  vector&lt;offset=mu_alpha, multiplier=sigma_alpha&gt;[J] alpha;  // non-centered parameterization\n  real beta;\n  real&lt;lower=0&gt; sigma;\n}\nmodel {\n  y ~ normal(alpha[county] + beta * x, sigma);\n  alpha ~ normal(mu_alpha, sigma_alpha); // partial-pooling\n  beta ~ normal(0, 10);\n  sigma ~ normal(0, 10);\n  mu_alpha ~ normal(0, 10);\n  sigma_alpha ~ normal(0, 10);\n}\ngenerated quantities {\n  array[N] real y_rep = normal_rng(alpha[county] + beta * x, sigma);\n}\n\"\"\"\n\nWe collect the dataset in the format that the stan model requires, and specify the dimensions of each of the non-scalar variables in the model:\n\ncounty_idx, counties = pd.factorize(radon_data[\"county\"], use_na_sentinel=False)\nobservations = radon_data.index\n\ncoords = {\n    \"county\": counties,\n    \"observation\": observations,\n}\n\ndims = {\n    \"alpha\": [\"county\"],\n    \"y_rep\": [\"observation\"],\n}\n\ndata = {\n    \"N\": len(observations),\n    \"J\": len(counties),\n    # Stan uses 1-based indexing!\n    \"county\": county_idx + 1,\n    \"x\": radon_data.log_uranium.values,\n    \"y\": radon_data.log_radon.values,\n}\n\nThen, we compile the model and provide the dimensions, coordinates and the dataset we just defined:\n\ncompiled_model = (\n    nutpie.compile_stan_model(code=model_code)\n    .with_data(**data)\n    .with_dims(**dims)\n    .with_coords(**coords)\n)\n\n\n%%time\ntrace = nutpie.sample(compiled_model, seed=0)\n\n\n\n\n\n\n\n    Sampler Progress\n    Total Chains: 6\n    Active Chains: 0\n    \n        Finished Chains:\n        6\n    \n    Sampling for now\n    \n        Estimated Time to Completion:\n        now\n    \n\n    \n    \n    \n        \n            \n                Progress\n                Draws\n                Divergences\n                Step Size\n                Gradients/Draw\n            \n        \n        \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    0.39\n                    31\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    0.47\n                    7\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    0.45\n                    7\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    0.46\n                    7\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    0.45\n                    7\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    0.45\n                    7\n                \n            \n            \n        \n    \n\n\n\nCPU times: user 2.27 s, sys: 39.2 ms, total: 2.31 s\nWall time: 547 ms\n\n\nAs some basic convergance checking we verify that all Rhat values are smaller than 1.02, all parameters have at least 500 effective draws and that we have no divergences:\n\nassert trace.sample_stats.diverging.sum() == 0\nassert az.ess(trace).min().min() &gt; 500\nassert az.rhat(trace).max().max() &gt; 1.02\n\nThanks to the coordinates and dimensions we specified, the resulting trace will now contain labeled data, so that plots based on it have properly set-up labels:\n\nimport arviz as az\nimport seaborn as sns\nimport xarray as xr\n\nsns.catplot(\n    data=trace.posterior.alpha.to_dataframe().reset_index(),\n    y=\"county\",\n    x=\"alpha\",\n    kind=\"boxen\",\n    height=13,\n    aspect=1/2.5,\n    showfliers=False,\n)"
  },
  {
    "objectID": "sample-stats.html",
    "href": "sample-stats.html",
    "title": "Understanding Sampler Statistics in Nutpie",
    "section": "",
    "text": "This guide explains the various statistics that nutpie collects during sampling. We’ll use Neal’s funnel distribution as an example, as it’s a challenging model that demonstrates many important sampling concepts."
  },
  {
    "objectID": "sample-stats.html#example-model-neals-funnel",
    "href": "sample-stats.html#example-model-neals-funnel",
    "title": "Understanding Sampler Statistics in Nutpie",
    "section": "Example Model: Neal’s Funnel",
    "text": "Example Model: Neal’s Funnel\nLet’s start by implementing Neal’s funnel in PyMC:\n\nimport pymc as pm\nimport nutpie\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport arviz as az\n\n# Create the funnel model\nwith pm.Model() as model:\n    log_sigma = pm.Normal('log_sigma')\n    pm.Normal('x', sigma=pm.math.exp(log_sigma), shape=5)\n\n# Sample with detailed statistics\ncompiled = nutpie.compile_pymc_model(model)\ntrace = nutpie.sample(\n    compiled,\n    tune=1000,\n    store_mass_matrix=True,\n    store_gradient=True,\n    store_unconstrained=True,\n    store_divergences=True,\n    seed=42,\n)\n\n\n\n\n\n\n\n    Sampler Progress\n    Total Chains: 6\n    Active Chains: 0\n    \n        Finished Chains:\n        6\n    \n    Sampling for now\n    \n        Estimated Time to Completion:\n        now\n    \n\n    \n    \n    \n        \n            \n                Progress\n                Draws\n                Divergences\n                Step Size\n                Gradients/Draw\n            \n        \n        \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    21\n                    0.41\n                    7\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    24\n                    0.35\n                    7\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.35\n                    7\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    13\n                    0.44\n                    7\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.34\n                    15\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    5\n                    0.53\n                    7"
  },
  {
    "objectID": "sample-stats.html#sampler-statistics-overview",
    "href": "sample-stats.html#sampler-statistics-overview",
    "title": "Understanding Sampler Statistics in Nutpie",
    "section": "Sampler Statistics Overview",
    "text": "Sampler Statistics Overview\nThe sampler statistics can be grouped into several categories:\n\nBasic HMC Statistics\nThese statistics are always collected and are essential for basic diagnostics:\n\n# Access through trace.sample_stats\nbasic_stats = [\n    'depth',              # Tree depth for current draw\n    'maxdepth_reached',   # Whether max tree depth was hit\n    'logp',               # Log probability of current position\n    'energy',             # Hamiltonian energy\n    'diverging',          # Whether the transition diverged\n    'step_size',          # Current step size\n    'step_size_bar',      # Current estimate of an ideal step size\n    'n_steps'             # Number of leapfrog steps\n\n]\n\n# Plot step size evolution during warmup\ntrace.warmup_sample_stats.step_size_bar.plot.line(x=\"draw\", yscale=\"log\")\n\n\n\n\n\n\n\n\n\n\nMass Matrix Adaptation\nThese statistics track how the mass matrix evolves:\n\n(\n    trace\n    .warmup_sample_stats\n    .mass_matrix_inv\n    .plot\n    .line(\n        x=\"draw\",\n        yscale=\"log\",\n        col=\"chain\",\n        col_wrap=2,\n    )\n)\n\n\n\n\n\n\n\n\nVariables that are a source of convergence issues, will often show high variance in the final mass matrix estimate across chains.\nThe mass matrix will always be fixed for 10% of draws at the end, because we only run final step size adaptation during that time, but high variance in the mass matrix before this final window and indicate that more tuning steps might be needed.\n\n\nDetailed Diagnostics\nThese are only available when explicitly requested:\ndetailed_stats = [\n    'gradient',              # Gradient at current position\n    'unconstrained_draw',    # Parameters in unconstrained space\n    'divergence_start',      # Position where divergence started\n    'divergence_end',        # Position where divergence ended\n    'divergence_momentum',   # Momentum at divergence\n    'divergence_message'     # Description of divergence\n]\n\nIdintify Divergences\nWe can for instance use this to identify the sources of divergences:\n\nimport xarray as xr\n\ndraws = (\n    trace\n    .sample_stats\n    .unconstrained_draw\n    .assign_coords(kind=\"draw\")\n)\ndivergence_locations = (\n    trace\n    .sample_stats\n    .divergence_start\n    .assign_coords(kind=\"divergence\")\n)\n\npoints = xr.concat([draws, divergence_locations], dim=\"kind\")\npoints.to_dataset(\"unconstrained_parameter\").plot.scatter(x=\"log_sigma\", y=\"x_0\", hue=\"kind\")\n\n\n\n\n\n\n\n\n\n\nCovariance of gradients and draws\nTODO this section should really use the transformed gradients and draws, not the unconstrained ones, as that avoids the manual mass matrix correction. This is only available for the normalizing flow adaptation at the moment though.\nIn models with problematic posterior correlations, the singular value decomposition of gradients and draws can often point us to the source of the issue.\nLet’s build a little model with correlations between parameters:\n\nwith pm.Model() as model:\n    x = pm.Normal('x')\n    y = pm.Normal(\"y\", mu=x, sigma=0.01)\n    z = pm.Normal(\"z\", mu=y, shape=100)\n\ncompiled = nutpie.compile_pymc_model(model)\ntrace = nutpie.sample(\n    compiled,\n    tune=1000,\n    store_gradient=True,\n    store_unconstrained=True,\n    store_mass_matrix=True,\n    seed=42,\n)\n\n\n\n\n\n\n\n    Sampler Progress\n    Total Chains: 6\n    Active Chains: 0\n    \n        Finished Chains:\n        6\n    \n    Sampling for now\n    \n        Estimated Time to Completion:\n        now\n    \n\n    \n    \n    \n        \n            \n                Progress\n                Draws\n                Divergences\n                Step Size\n                Gradients/Draw\n            \n        \n        \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.13\n                    31\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.16\n                    31\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.22\n                    15\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.14\n                    31\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.15\n                    31\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.15\n                    31\n                \n            \n            \n        \n    \n\n\n\nNow we can compute eigenvalues of the covariance matrix of the gradient and draws (using the singular value decomposition to avoid quadratic cost):\n\ndef covariance_eigenvalues(x, mass_matrix):\n    assert x.dims == (\"chain\", \"draw\", \"unconstrained_parameter\")\n    x = x.stack(sample=[\"draw\", \"chain\"])\n    x = (x - x.mean(\"sample\")) / np.sqrt(mass_matrix)\n    u, s, v = np.linalg.svd(x.T / np.sqrt(x.shape[1]), full_matrices=False)\n    print(u.shape, s.shape, v.shape)\n    s = xr.DataArray(\n        s,\n        dims=[\"eigenvalue\"],\n        coords={\"eigenvalue\": range(s.size)},\n    )\n    v = xr.DataArray(\n        v,\n        dims=[\"eigenvalue\", \"unconstrained_parameter\"],\n        coords={\n            \"eigenvalue\": s.eigenvalue,\n            \"unconstrained_parameter\": x.unconstrained_parameter,\n        },\n    )\n    return s ** 2, v\n\nmass_matrix = trace.sample_stats.mass_matrix_inv.isel(draw=-1, chain=0)\ndraws_eigs, draws_eigv = covariance_eigenvalues(trace.sample_stats.unconstrained_draw, mass_matrix)\ngrads_eigs, grads_eigv = covariance_eigenvalues(trace.sample_stats.gradient, 1 / mass_matrix)\n\ndraws_eigs.plot.line(x=\"eigenvalue\", yscale=\"log\")\ngrads_eigs.plot.line(x=\"eigenvalue\", yscale=\"log\")\n\n(6000, 102) (102,) (102, 102)\n(6000, 102) (102,) (102, 102)\n\n\n\n\n\n\n\n\n\nWe can see one very large and one very small eigenvalue in both covariances. Large eigenvalues for the draws, and small eigenvalues for the gradients prevent the sampler from taking larger steps. Small eigenvalues in the draws, and large eigenvalues in the grads mean, that the sampler has to move far in parameter space to get independent draws. So both lead to problems during sampling. For models with many parameters, typically only the large eigenvalues of each are meaningful, because of estimation issues with the small eigenvalues.\nWe can also look at the eigenvectors to see which parameters are responsible for the correlations:\n\n(\n    draws_eigv\n    .sel(eigenvalue=0)\n    .to_pandas()\n    .sort_values(key=abs)\n    .tail(10)\n    .plot.bar(x=\"unconstrained_parameter\")\n)\n\n\n\n\n\n\n\n\n\n(\n    grads_eigv\n    .sel(eigenvalue=0)\n    .to_pandas()\n    .sort_values(key=abs)\n    .tail(10)\n    .plot.bar(x=\"unconstrained_parameter\")\n)"
  },
  {
    "objectID": "nf-adapt.html",
    "href": "nf-adapt.html",
    "title": "Adaptation with Normalizing Flows",
    "section": "",
    "text": "Experimental and subject to change\nNormalizing flow adaptation through Fisher HMC is a new sampling algorithm that automatically reparameterizes a model. It adds some computational cost outside model log-density evaluations, but allows sampling from much more difficult posterior distributions. For models with expensive log-density evaluations, the normalizing flow adaptation can also be much faster, if it can reduce the number of log-density evaluations needed to reach a given effective sample size.\nThe normalizing flow adaptation works by learning a transformation of the parameter space that makes the posterior distribution more amenable to sampling. This is done by fitting a sequence of invertible transformations (the “flow”) that maps the original parameter space to a space where the posterior is closer to a standard normal distribution. The flow is trained during warmup.\nFor more information about the algorithm, see the (still work in progress) paper If only my posterior were normal: Introducing Fisher HMC.\nCurrently, a lot of time is spent on compiling various parts of the normalizing flow, and for small models this can take a large amount of the total time. Hopefully, we will be able to reduce this overhead in the future."
  },
  {
    "objectID": "nf-adapt.html#requirements",
    "href": "nf-adapt.html#requirements",
    "title": "Adaptation with Normalizing Flows",
    "section": "Requirements",
    "text": "Requirements\nInstall the optional dependencies for normalizing flow adaptation:\npip install 'nutpie[nnflow]'\nIf you use with PyMC, this will only work if the model is compiled using the jax backend, and if the gradient_backend is also set to jax.\nTraining of the normalizing flow can often be accelerated by using a GPU (even if the model itself is written in Stan, without any GPU support). To enable GPU you need to make sure your jax installation comes with GPU support, for instance by installing it with pip install 'jax[cuda12]', or selecting the jaxlib version with GPU support, if you are using conda-forge. You can check if your installation has GPU support by checking the output of:\nimport jax\njax.devices()\n\nUsage\nTo use normalizing flow adaptation in nutpie, you need to enable the transform_adapt option during sampling. Here is an example of how we can use it to sample from a difficult posterior:\n\nimport pymc as pm\nimport nutpie\nimport numpy as np\nimport arviz\n\n# Define a 100-dimensional funnel model\nwith pm.Model() as model:\n    log_sigma = pm.Normal(\"log_sigma\")\n    pm.Normal(\"x\", mu=0, sigma=pm.math.exp(log_sigma / 2), shape=100)\n\n# Compile the model with the jax backend\ncompiled = nutpie.compile_pymc_model(\n    model, backend=\"jax\", gradient_backend=\"jax\"\n)\n\nIf we sample this model without normalizing flow adaptation, we will encounter convergence issues, often divergences and always low effective sample sizes:\n\n# Sample without normalizing flow adaptation\ntrace_no_nf = nutpie.sample(compiled, seed=1)\nassert (arviz.ess(trace_no_nf) &lt; 100).any().to_array().any()\n\n\n\n\n\n\n\n    Sampler Progress\n    Total Chains: 6\n    Active Chains: 0\n    \n        Finished Chains:\n        6\n    \n    Sampling for 16 seconds\n    \n        Estimated Time to Completion:\n        now\n    \n\n    \n    \n    \n        \n            \n                Progress\n                Draws\n                Divergences\n                Step Size\n                Gradients/Draw\n            \n        \n        \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    0.45\n                    7\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    0.31\n                    15\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    0.31\n                    7\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    0.28\n                    7\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    0.39\n                    15\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    0.34\n                    7\n                \n            \n            \n        \n    \n\n\n\n\n# We can add further arguments for the normalizing flow:\ncompiled = compiled.with_transform_adapt(\n    num_layers=5,        # Number of layers in the normalizing flow\n    nn_width=32,         # Neural networks with 32 hidden units\n    num_diag_windows=6,  # Number of windows with a diagonal mass matrix intead of a flow\n    verbose=False,       # Whether to print details about the adaptation process\n    show_progress=False, # Whether to show a progress bar for each optimization step\n)\n\n# Sample with normalizing flow adaptation\ntrace_nf = nutpie.sample(\n    compiled,\n    transform_adapt=True,  # Enable the normalizing flow adaptation\n    seed=1,\n    chains=2,\n    cores=1,  # Running chains in parallel can be slow\n    window_switch_freq=150,  # Optimize the normalizing flow every 150 iterations\n)\nassert trace_nf.sample_stats.diverging.sum() == 0\nassert (arviz.ess(trace_nf) &gt; 1000).all().to_array().all()\n\n\n\n\n\n\n\n    Sampler Progress\n    Total Chains: 2\n    Active Chains: 0\n    \n        Finished Chains:\n        2\n    \n    Sampling for 18 minutes\n    \n        Estimated Time to Completion:\n        now\n    \n\n    \n    \n    \n        \n            \n                Progress\n                Draws\n                Divergences\n                Step Size\n                Gradients/Draw\n            \n        \n        \n            \n                \n                    \n                        \n                        \n                    \n                    2500\n                    0\n                    0.52\n                    7\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2500\n                    0\n                    0.53\n                    7\n                \n            \n            \n        \n    \n\n\n\nThe sampler used fewer gradient evaluations with the normalizing flow adaptation, but still converged, and produce a good effective sample size:\n\nn_steps = int(trace_nf.sample_stats.n_steps.sum() + trace_nf.warmup_sample_stats.n_steps.sum())\ness = float(arviz.ess(trace_nf).min().to_array().min())\nprint(f\"Number of gradient evaluations: {n_steps}\")\nprint(f\"Minimum effective sample size: {ess}\")\n\nNumber of gradient evaluations: 42527\nMinimum effective sample size: 1835.9674640023168\n\n\nWithout normalizing flow, it used more gradient evaluations, and still wasn’t able to get a good effective sample size:\n\nn_steps = int(trace_no_nf.sample_stats.n_steps.sum() + trace_no_nf.warmup_sample_stats.n_steps.sum())\ness = float(arviz.ess(trace_no_nf).min().to_array().min())\nprint(f\"Number of gradient evaluations: {n_steps}\")\nprint(f\"Minimum effective sample size: {ess}\")\n\nNumber of gradient evaluations: 124219\nMinimum effective sample size: 31.459420094540565\n\n\nThe flow adaptation occurs during warmup, so the number of warmup draws should be large enough to allow the flow to converge. For more complex posteriors, you may need to increase the number of layers (using the num_layers argument), or you might want to increase the number of warmup draws.\nTo monitor the progress of the flow adaptation, you can set verbose=True, or show_progress=True, but the second should only be used if you sample just one chain.\nAll losses are on a log-scale. Negative values smaller -2 are a good sign that the adaptation was successful. If the loss stays positive, the flow is either not expressive enough, or the training period is too short. The sampler might still converge, but will probably need more gradient evaluations per effective draw. Large losses bigger than 6 tend to indicate that the posterior is too difficult to sample with the current flow, and the sampler will probably not converge."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Nutpie is part of the PyMC organization. The PyMC organization develops and maintains tools for Bayesian statistical modeling and probabilistic machine learning.\nNutpie provides a high-performance implementation of the No-U-Turn Sampler (NUTS) that can be used with models defined in PyMC, Stan and other frameworks. It was created to enable faster and more efficient Bayesian inference while maintaining compatibility with existing probabilistic programming tools.\nFor more information about the PyMC organization, visit the following links:\n\nPyMC Website\nPyMC GitHub Organization"
  },
  {
    "objectID": "benchmarks.html",
    "href": "benchmarks.html",
    "title": "Benchmarks",
    "section": "",
    "text": "Benchmarks"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nutpie Documentation",
    "section": "",
    "text": "nutpie is a high-performance library designed for Bayesian inference, that provides efficient sampling algorithms for probabilistic models. It can sample models that are defined in PyMC or Stan (numpyro and custom hand-coded likelihoods with gradient are coming soon)."
  },
  {
    "objectID": "index.html#quickstart-pymc",
    "href": "index.html#quickstart-pymc",
    "title": "Nutpie Documentation",
    "section": "Quickstart: PyMC",
    "text": "Quickstart: PyMC\nInstall nutpie with pip, uv, pixi, or conda:\nFor usage with pymc:\n# One of\npip install \"nutpie[pymc]\"\nuv add \"nutpie[pymc]\"\npixi add nutpie pymc numba\nconda install -c conda-forge nutpie pymc numba\nAnd then sample with\n\nimport nutpie\nimport pymc as pm\n\nwith pm.Model() as model:\n    mu = pm.Normal(\"mu\", mu=0, sigma=1)\n    obs = pm.Normal(\"obs\", mu=mu, sigma=1, observed=[1, 2, 3])\n\ncompiled = nutpie.compile_pymc_model(model)\ntrace = nutpie.sample(compiled)\n\n\n\n\n\n\n\n    Sampler Progress\n    Total Chains: 6\n    Active Chains: 0\n    \n        Finished Chains:\n        6\n    \n    Sampling for now\n    \n        Estimated Time to Completion:\n        now\n    \n\n    \n    \n    \n        \n            \n                Progress\n                Draws\n                Divergences\n                Step Size\n                Gradients/Draw\n            \n        \n        \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    1.35\n                    1\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    1.28\n                    3\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    1.29\n                    3\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    1.23\n                    3\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    1.40\n                    3\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    1.28\n                    1\n                \n            \n            \n        \n    \n\n\n\nFor more information, see the detailed PyMC usage guide."
  },
  {
    "objectID": "index.html#quickstart-stan",
    "href": "index.html#quickstart-stan",
    "title": "Nutpie Documentation",
    "section": "Quickstart: Stan",
    "text": "Quickstart: Stan\nStan needs access to a compiler toolchain, you can find instructions for those here. You can then install nutpie through pip or uv:\n# One of\npip install \"nutpie[stan]\"\nuv add \"nutpie[stan]\"\n\nimport nutpie\n\nmodel = \"\"\"\ndata {\n    int&lt;lower=0&gt; N;\n    vector[N] y;\n}\nparameters {\n    real mu;\n}\nmodel {\n    mu ~ normal(0, 1);\n    y ~ normal(mu, 1);\n}\n\"\"\"\n\ncompiled = (\n    nutpie\n    .compile_stan_model(code=model)\n    .with_data(N=3, y=[1, 2, 3])\n)\ntrace = nutpie.sample(compiled)\n\n\n\n\n\n\n\n    Sampler Progress\n    Total Chains: 6\n    Active Chains: 0\n    \n        Finished Chains:\n        6\n    \n    Sampling for now\n    \n        Estimated Time to Completion:\n        now\n    \n\n    \n    \n    \n        \n            \n                Progress\n                Draws\n                Divergences\n                Step Size\n                Gradients/Draw\n            \n        \n        \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    1.29\n                    1\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    1.27\n                    3\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    1.34\n                    3\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    1.33\n                    1\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    1.41\n                    3\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    1.29\n                    3\n                \n            \n            \n        \n    \n\n\n\nFor more information, see the detailed Stan usage guide."
  },
  {
    "objectID": "pymc-usage.html",
    "href": "pymc-usage.html",
    "title": "Usage with PyMC models",
    "section": "",
    "text": "This document shows how to use nutpie with PyMC models. We will use the pymc package to define a simple model and sample from it using nutpie."
  },
  {
    "objectID": "pymc-usage.html#installation",
    "href": "pymc-usage.html#installation",
    "title": "Usage with PyMC models",
    "section": "Installation",
    "text": "Installation\nThe recommended way to install pymc is through the conda ecosystem. A good package manager for conda packages is pixi. See for the pixi documentation for instructions on how to install it.\nWe create a new project for this example:\npixi new pymc-example\nThis will create a new directory pymc-example with a pixi.toml file, that you can edit to add meta information.\nWe then add the pymc and nutpie packages to the project:\ncd pymc-example\npixi add pymc nutpie arviz\nYou can use Visual Studio Code (VSCode) or JupyterLab to write and run our code. Both are excellent tools for working with Python and data science projects.\n\nUsing VSCode\n\nOpen VSCode.\nOpen the pymc-example directory created earlier.\nCreate a new file named model.ipynb.\nSelect the pixi kernel to run the code.\n\n\n\nUsing JupyterLab\n\nAdd jupyter labs to the project by running pixi add jupyterlab.\nOpen JupyterLab by running pixi run jupyter lab in your terminal.\nCreate a new Python notebook."
  },
  {
    "objectID": "pymc-usage.html#defining-and-sampling-a-simple-model",
    "href": "pymc-usage.html#defining-and-sampling-a-simple-model",
    "title": "Usage with PyMC models",
    "section": "Defining and Sampling a Simple Model",
    "text": "Defining and Sampling a Simple Model\nWe will define a simple Bayesian model using pymc and sample from it using nutpie.\n\nModel Definition\nIn your model.ipypy file or Jupyter notebook, add the following code:\n\nimport pymc as pm\nimport nutpie\nimport pandas as pd\n\ncoords = {\"observation\": range(3)}\n\nwith pm.Model(coords=coords) as model:\n    # Prior distributions for the intercept and slope\n    intercept = pm.Normal(\"intercept\", mu=0, sigma=1)\n    slope = pm.Normal(\"slope\", mu=0, sigma=1)\n\n    # Likelihood (sampling distribution) of observations\n    x = [1, 2, 3]\n\n    mu = intercept + slope * x\n    y = pm.Normal(\"y\", mu=mu, sigma=0.1, observed=[1, 2, 3], dims=\"observation\")\n\n\n\nSampling\nWe can now compile the model using the numba backend:\n\ncompiled = nutpie.compile_pymc_model(model)\ntrace = nutpie.sample(compiled)\n\n\n\n\n\n\n\n    Sampler Progress\n    Total Chains: 6\n    Active Chains: 0\n    \n        Finished Chains:\n        6\n    \n    Sampling for now\n    \n        Estimated Time to Completion:\n        now\n    \n\n    \n    \n    \n        \n            \n                Progress\n                Draws\n                Divergences\n                Step Size\n                Gradients/Draw\n            \n        \n        \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    0.59\n                    3\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    0.65\n                    9\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    0.55\n                    1\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    0.58\n                    15\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    0.67\n                    7\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    0.58\n                    3\n                \n            \n            \n        \n    \n\n\n\nAlternatively, we can also sample through the pymc API:\nwith model:\n    trace = pm.sample(model, nuts_sampler=\"nutpie\")\nWhile sampling, nutpie shows a progress bar for each chain. It also includes information about how each chain is doing:\n\nIt shows the current number of draws\nThe step size of the integrator (very small stepsizes are typically a bad sign)\nThe number of divergences (if there are divergences, that means that nutpie is probably not sampling the posterior correctly)\nThe number of gradient evaluation nutpie uses for each draw. Large numbers (100 to 1000) are a sign that the parameterization of the model is not ideal, and the sampler is very inefficient.\n\nAfter sampling, this returns an arviz InferenceData object that you can use to analyze the trace.\nFor example, we should check the effective sample size:\n\nimport arviz as az\naz.ess(trace)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 16B\nDimensions:    ()\nData variables:\n    intercept  float64 8B 1.517e+03\n    slope      float64 8B 1.517e+03xarray.DatasetDimensions:Coordinates: (0)Data variables: (2)intercept()float641.517e+03array(1517.4729764)slope()float641.517e+03array(1517.44085063)Indexes: (0)Attributes: (0)\n\n\nand take a look at a trace plot:\n\naz.plot_trace(trace);\n\n\n\n\n\n\n\n\n\n\nChoosing the backend\nRight now, we have been using the numba backend. This is the default backend for nutpie, when sampling from pymc models. It tends to have relatively long compilation times, but samples small models very efficiently. For larger models the jax backend sometimes outperforms numba.\nFirst, we need to install the jax package:\npixi add jax\nWe can select the backend by passing the backend argument to the compile_pymc_model:\ncompiled_jax = nutpie.compiled_pymc_model(model, backend=\"jax\")\ntrace = nutpie.sample(compiled_jax)\nOr through the pymc API:\nwith model:\n    trace = pm.sample(\n        model,\n        nuts_sampler=\"nutpie\",\n        nuts_sampler_kwargs={\"backend\": \"jax\"},\n    )\nIf you have an nvidia GPU, you can also use the jax backend with the gpu. We will have to install the jaxlib package with the cuda option\npixi add jaxlib --build 'cuda12'\nRestart the kernel and check that the GPU is available:\nimport jax\n\n# Should list the cuda device\njax.devices()\nSampling again, should now use the GPU, which you can observe by checking the GPU usage with nvidia-smi or nvtop.\n\n\nChanging the dataset without recompilation\nIf you want to use the same model with different datasets, you can modify datasets after compilation. Since jax does not like changes in shapes, this is only recommended with the numba backend.\nFirst, we define the model, but put our dataset in a pm.Data structure:\n\nwith pm.Model() as model:\n    x = pm.Data(\"x\", [1, 2, 3])\n    intercept = pm.Normal(\"intercept\", mu=0, sigma=1)\n    slope = pm.Normal(\"slope\", mu=0, sigma=1)\n    mu = intercept + slope * x\n    y = pm.Normal(\"y\", mu=mu, sigma=0.1, observed=[1, 2, 3])\n\nWe can now compile the model:\n\ncompiled = nutpie.compile_pymc_model(model)\ntrace = nutpie.sample(compiled)\n\n\n\n\n\n\n\n    Sampler Progress\n    Total Chains: 6\n    Active Chains: 0\n    \n        Finished Chains:\n        6\n    \n    Sampling for now\n    \n        Estimated Time to Completion:\n        now\n    \n\n    \n    \n    \n        \n            \n                Progress\n                Draws\n                Divergences\n                Step Size\n                Gradients/Draw\n            \n        \n        \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    0.64\n                    1\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    0.61\n                    11\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    0.68\n                    3\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    0.55\n                    9\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    0.57\n                    9\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    0.65\n                    3\n                \n            \n            \n        \n    \n\n\n\nAfter compilation, we can change the dataset:\n\ncompiled2 = compiled.with_data(x=[4, 5, 6])\ntrace2 = nutpie.sample(compiled2)\n\n\n\n\n\n\n\n    Sampler Progress\n    Total Chains: 6\n    Active Chains: 0\n    \n        Finished Chains:\n        6\n    \n    Sampling for now\n    \n        Estimated Time to Completion:\n        now\n    \n\n    \n    \n    \n        \n            \n                Progress\n                Draws\n                Divergences\n                Step Size\n                Gradients/Draw\n            \n        \n        \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    0.42\n                    7\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    0.35\n                    27\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    0.42\n                    13\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    0.45\n                    3\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    0.40\n                    3\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1400\n                    0\n                    0.39\n                    19"
  },
  {
    "objectID": "sampling-options.html",
    "href": "sampling-options.html",
    "title": "Sampling Configuration Guide",
    "section": "",
    "text": "This guide covers the configuration options for nutpie.sample and provides practical advice for tuning your sampler. We’ll start with basic usage and move to advanced topics like mass matrix adaptation.\n\n\nFor most models, don’t think too much about the options of the sampler, and just use the defaults. Most sampling problems can’t easily be solved by changing the sampler, most of the time they require model changes. So in most cases, simply use\ntrace = nutpie.sample(compiled_model)\n\n\n\n\n\ntrace = nutpie.sample(\n    model,\n    draws=1000,          # Number of post-warmup draws per chain\n    tune=500,            # Number of warmup draws for adaptation\n    chains=6,            # Number of independent chains\n    cores=None,          # Number chains that are allowed to run simultainiously\n    seed=12345          # Random seed for reproducibility\n)\nThe number of draws affects both accuracy and computational cost: - Too few draws (&lt; 500) may not capture the posterior well - Too many draws (&gt; 10000) may waste computation time\nIf a model is sampling without divergences, but with effective sample sizes that are not as large as necessary to accieve the markov-error for your estimates, you can increase the number of chains and/or draws.\nIf the effective sample size is much smaller than the number of draws, you might want to consider reparameterizing the model instead, to for instance remove posterior correlations.\n\n\n\n\nYou can enable more detailed diagnostics when troubleshooting:\ntrace = nutpie.sample(\n    model,\n    save_warmup=True,          # Keep warmup draws, default is True\n    store_divergences=True,    # Track divergent transitions\n    store_unconstrained=True,  # Store transformed parameters\n    store_gradient=True,       # Store gradient information\n    store_mass_matrix=True     # Track mass matrix adaptation\n)\nFor each of the store_* arguments, additional arrays will be availbale in the trace.sample_stats.\n\n\n\n\n\ntrace = nutpie.sample(\n    model,\n    target_accept=0.8,     # Target acceptance rate\n    maxdepth=10            # Maximum tree depth\n    max_energy_error=1000  # Error at witch to count the trajectory as a divergent transition\n)\nThe target_accept parameter implicitly controls the step size of the leapfrog steps in the HMC sampler. During tuning, the sampler will try to choose a step size, such that the acceptance statistic is target_accept. It has to be between 0 and 1.\nThe default is 0.8. Larger values will increase the computational cost, but might avoid divergences during sampling. In many diverging models increasing target_accept will only make divergences less frequent however, and not solve the underlying problem.\nLowering the maximum energy error to for instance 10 will often increase the number of divergences, and make it easier to diagnose their cause. With lower value the divergences often are reported closer to the critical points in the parameter space, where the model is most likely to diverge.\n\n\n\n\nNutpie offers several strategies for adapting the mass matrix, which determines how the sampler navigates the parameter space.\n\n\nBy setting use_grad_based_mass_matrix=False, the sampling algorithm will more closely resemble the algorithm in Stan and PyMC. Usually, this will result in less efficient sampling, but the total number of effective samples is sometimes higher. If this is set to True (the default), nutpie will use diagonal mass matrix estimates that are based on the posterior draws and the scores at those positions.\ntrace = nutpie.sample(\n    model,\n    use_grad_based_mass_matrix=False\n)\n\n\n\nFor models with strong parameter correlations you can enable a low rank modified mass matrix. The mass_matrix_gamma parameter is a regularization parameter. More regularization will lead to a smaller effect of the low-rank components, but might work better for hiegher dimensional problems.\nmass_matrix_eigval_cutoff should be greater than one, and controls how large an eigenvalue of the full mass matrix has to be, to be included into the low-rank mass matirx.\ntrace = nutpie.sample(\n    model,\n    low_rank_modified_mass_matrix=True,\n    mass_matrix_eigval_cutoff=3,\n    mass_matrix_gamma=1e-5\n)\n\n\n\ntrasform_adapt is an experimental feature that allows sampling from many posteriors, where current methods diverge. It is described in more detail here.\ntrace = nutpie.sample(\n    model,\n    transform_adapt=True  # Experimental reparameterization\n)\n\n\n\n\nCustomize the sampling progress display:\ntrace = nutpie.sample(\n    model,\n    progress_bar=True,\n    progress_rate=500,  # Update every 500ms\n)"
  },
  {
    "objectID": "sampling-options.html#quick-start",
    "href": "sampling-options.html#quick-start",
    "title": "Sampling Configuration Guide",
    "section": "",
    "text": "For most models, don’t think too much about the options of the sampler, and just use the defaults. Most sampling problems can’t easily be solved by changing the sampler, most of the time they require model changes. So in most cases, simply use\ntrace = nutpie.sample(compiled_model)"
  },
  {
    "objectID": "sampling-options.html#core-sampling-parameters",
    "href": "sampling-options.html#core-sampling-parameters",
    "title": "Sampling Configuration Guide",
    "section": "",
    "text": "trace = nutpie.sample(\n    model,\n    draws=1000,          # Number of post-warmup draws per chain\n    tune=500,            # Number of warmup draws for adaptation\n    chains=6,            # Number of independent chains\n    cores=None,          # Number chains that are allowed to run simultainiously\n    seed=12345          # Random seed for reproducibility\n)\nThe number of draws affects both accuracy and computational cost: - Too few draws (&lt; 500) may not capture the posterior well - Too many draws (&gt; 10000) may waste computation time\nIf a model is sampling without divergences, but with effective sample sizes that are not as large as necessary to accieve the markov-error for your estimates, you can increase the number of chains and/or draws.\nIf the effective sample size is much smaller than the number of draws, you might want to consider reparameterizing the model instead, to for instance remove posterior correlations."
  },
  {
    "objectID": "sampling-options.html#sampler-diagnostics",
    "href": "sampling-options.html#sampler-diagnostics",
    "title": "Sampling Configuration Guide",
    "section": "",
    "text": "You can enable more detailed diagnostics when troubleshooting:\ntrace = nutpie.sample(\n    model,\n    save_warmup=True,          # Keep warmup draws, default is True\n    store_divergences=True,    # Track divergent transitions\n    store_unconstrained=True,  # Store transformed parameters\n    store_gradient=True,       # Store gradient information\n    store_mass_matrix=True     # Track mass matrix adaptation\n)\nFor each of the store_* arguments, additional arrays will be availbale in the trace.sample_stats."
  },
  {
    "objectID": "sampling-options.html#non-blocking-sampling",
    "href": "sampling-options.html#non-blocking-sampling",
    "title": "Sampling Configuration Guide",
    "section": "",
    "text": "trace = nutpie.sample(\n    model,\n    target_accept=0.8,     # Target acceptance rate\n    maxdepth=10            # Maximum tree depth\n    max_energy_error=1000  # Error at witch to count the trajectory as a divergent transition\n)\nThe target_accept parameter implicitly controls the step size of the leapfrog steps in the HMC sampler. During tuning, the sampler will try to choose a step size, such that the acceptance statistic is target_accept. It has to be between 0 and 1.\nThe default is 0.8. Larger values will increase the computational cost, but might avoid divergences during sampling. In many diverging models increasing target_accept will only make divergences less frequent however, and not solve the underlying problem.\nLowering the maximum energy error to for instance 10 will often increase the number of divergences, and make it easier to diagnose their cause. With lower value the divergences often are reported closer to the critical points in the parameter space, where the model is most likely to diverge."
  },
  {
    "objectID": "sampling-options.html#mass-matrix-adaptation",
    "href": "sampling-options.html#mass-matrix-adaptation",
    "title": "Sampling Configuration Guide",
    "section": "",
    "text": "Nutpie offers several strategies for adapting the mass matrix, which determines how the sampler navigates the parameter space.\n\n\nBy setting use_grad_based_mass_matrix=False, the sampling algorithm will more closely resemble the algorithm in Stan and PyMC. Usually, this will result in less efficient sampling, but the total number of effective samples is sometimes higher. If this is set to True (the default), nutpie will use diagonal mass matrix estimates that are based on the posterior draws and the scores at those positions.\ntrace = nutpie.sample(\n    model,\n    use_grad_based_mass_matrix=False\n)\n\n\n\nFor models with strong parameter correlations you can enable a low rank modified mass matrix. The mass_matrix_gamma parameter is a regularization parameter. More regularization will lead to a smaller effect of the low-rank components, but might work better for hiegher dimensional problems.\nmass_matrix_eigval_cutoff should be greater than one, and controls how large an eigenvalue of the full mass matrix has to be, to be included into the low-rank mass matirx.\ntrace = nutpie.sample(\n    model,\n    low_rank_modified_mass_matrix=True,\n    mass_matrix_eigval_cutoff=3,\n    mass_matrix_gamma=1e-5\n)\n\n\n\ntrasform_adapt is an experimental feature that allows sampling from many posteriors, where current methods diverge. It is described in more detail here.\ntrace = nutpie.sample(\n    model,\n    transform_adapt=True  # Experimental reparameterization\n)"
  },
  {
    "objectID": "sampling-options.html#progress-monitoring",
    "href": "sampling-options.html#progress-monitoring",
    "title": "Sampling Configuration Guide",
    "section": "",
    "text": "Customize the sampling progress display:\ntrace = nutpie.sample(\n    model,\n    progress_bar=True,\n    progress_rate=500,  # Update every 500ms\n)"
  }
]